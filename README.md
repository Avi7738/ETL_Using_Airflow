# ETL_Using_Airflow

# ğŸŒ€ News ETL Pipeline with Apache Airflow + Snowflake + Docker

This project builds an end-to-end automated pipeline to extract real-time tech news using a public API, transform the content, and load the data into a Snowflake table â€” all orchestrated using Apache Airflow inside Docker.

---

## ğŸ“¦ Tech Stack

| Tool        | Role                          |
|-------------|-------------------------------|
| ğŸ³ Docker    | Containerization & orchestration |
| ğŸŒ€ Airflow   | Workflow scheduler for ETL     |
| ğŸ Python    | Logic: API call, JSON transform |
| â„ï¸ Snowflake | Cloud data warehouse           |
| ğŸ“Š Power BI  | Dashboard from Snowflake       |

---

## ğŸ§± Project Structure

```bash
news-etl-airflow/
â”œâ”€â”€ dags/                    # Airflow DAGs
â”‚   â””â”€â”€ news_etl_to_snowflake.py
â”œâ”€â”€ config/                  # Optional configs
â”œâ”€â”€ logs/                    # Airflow logs (auto-created)
â”œâ”€â”€ plugins/                 # (empty or custom plugins)
â”œâ”€â”€ .env                     # Secret env variables (DO NOT commit)
â”œâ”€â”€ .gitignore               # Ignore .env, logs etc.
â”œâ”€â”€ docker-compose.yml       # Main Docker setup
â””â”€â”€ README.md                # This file
```

#ğŸš€ How to Run the Project
## âœ… Step 1: Clone the Repository

git clone https://github.com/YOUR_USERNAME/news-etl-airflow.git
cd news-etl-airflow

## âœ… Step 2: Add .env File
Create a .env file in the root as shown above with your News API key and Snowflake credentials.

## âœ… Step 3: Start Docker Environment

docker-compose up airflow-init     # Run once to initialize metadata DB
docker-compose up -d               # Starts Airflow webserver & scheduler

## âœ… Step 4: Open Airflow UI
Go to http://localhost:8080
Login with:

Username: airflow
Password: airflow

## âœ… Step 5: Trigger the ETL DAG

In the Airflow UI, locate DAG news_etl_to_snowflake

Turn it ON

Click â–¶ï¸ Trigger DAG

Monitor logs to confirm success

# ğŸŒ€ Airflow DAG Workflow
This DAG automates the following ETL steps:

Step	Task ID	Description
Extract news	extract_news	Calls News API for latest tech headlines
Transform data	transform_news	Cleans HTML tags and prepares JSON
Load to Snowflake	load_to_snowflake	Creates table (if not exists) and inserts rows

# â„ï¸ Snowflake Table: TECH_NEWS
Column	Type	Description
TITLE	STRING	News article title
DESCRIPTION	STRING	News summary
URL	STRING	Link to the original article
PUBLISHED_AT	TIMESTAMP	Date/time of article publication

ğŸ“Š Power BI Integration
You can connect Power BI directly to Snowflake to build real-time dashboards.

ğŸ§© Steps to Connect:
Open Power BI Desktop

Click Get Data â†’ Snowflake

Fill the following fields:

Field	Value
Server	nscziip-bj25145.snowflakecomputing.com
Warehouse	AIRFLOW_WH
Database	AIRFLOW_DB
Schema	PUBLIC

Select the table: TECH_NEWS

Choose DirectQuery for live updates

Build visuals:

ğŸ“‹ Table: news headlines + links

ğŸ“ˆ Line chart: published_at vs count

ğŸ§  Add filters by title/description/date

ğŸ’¡ Future Improvements
Add NLP sentiment analysis using HuggingFace or Spark NLP

Load enriched data into TECH_NEWS_ENRICHED

Add alerting (Slack/Email) for specific keyword-based news

Add Streamlit or Flask dashboard on top of Snowflake

Switch from batch to real-time Kafka ingestion

ğŸ‘¨â€ğŸ’» Author
Avinash Rajbhar
Data Engineer | ETL & Dashboard Automator | Real-Time Pipeline Builder
Made with â¤ï¸ using Docker, Python, Airflow & Snowflake

yaml
Copy
Edit

---

âœ… Ab bhai tu yeh full `README.md` copy-paste kar de GitHub repo me â€” tera project ready hai interview, demo, or showcase ke liye ğŸ’¯

Bol â€” chahiye kya:
- `.env.example`
- `docker-compose.yml`
- DAG file cleanup
- Power BI `.pbix` file template?

Main ready hoon ğŸ’¥
